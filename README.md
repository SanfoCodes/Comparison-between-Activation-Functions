
# ğŸ” Comparing Activation Functions in Neural Networks

This repository contains the full codebase, analysis, visualizations, and theoretical evaluation for the project:

**"Mathematical Analysis and Theoretical Trade-offs of Activation Functions in Neural Networks: Convergence Behavior and Gradient Dynamics"**

Developed as part of the DASC 5420 course at TRU.

---

## ğŸ“š Project Summary

This project evaluates and compares the performance of five common activation functions:

- **ReLU**
- **Sigmoid**
- **Tanh**
- **Leaky ReLU**
- **Swish**

We investigate their:
- Convergence speed
- Generalization (accuracy)
- Gradient dynamics (vanishing/exploding)
- Mathematical properties (smoothness, monotonicity, saturation)

---

## Structure

## ğŸ“ Repository Structure

```text
.
â”œâ”€â”€ modelling/              # Model architecture, training loop, data loaders
â”‚   â”œâ”€â”€ model_builder.py
â”‚   â”œâ”€â”€ trainer.py
â”‚   â””â”€â”€ utils.py
â”‚
â”œâ”€â”€ visualization/          # Plotting tools for loss, accuracy, gradients, derivatives
â”‚   â”œâ”€â”€ loss_plots.py
â”‚   â”œâ”€â”€ gradient_flow.py
â”‚   â”œâ”€â”€ activation_derivatives.py
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ presentation/           # Report, math notes, summaries 
â”‚   â””â”€â”€ activation_analysis.md
â”‚
â”œâ”€â”€ main.ipynb              # Main Colab notebook: trains all models, runs all visualizations
â””â”€â”€ README.md               # You are here ğŸš€


---

## ğŸ§ª How to Run (Google Colab)

1. Upload this repo or clone it to Google Colab.
2. Make sure CIFAR-10 can be downloaded (via `torchvision.datasets`).
3. Run `main.ipynb` to:
   - Train models with all 5 activation functions
   - Generate and save all loss/accuracy/gradient/derivative plots

---

## ğŸ§  Theory + Derivations

This repo integrates:
- Formal conditions for faster convergence (e.g., Lipschitz continuity)
- Optimization landscape insights (plateaus, curvature, gradient norms)
- Activation derivative analysis (impact on gradient flow)
- Key theorems from the **TeLU paper** on smooth approximations of ReLU

See `presentation/activation_analysis.md` for all derivations and theoretical write-up.

---

## ğŸ’¾ Output Plots

All generated plots are saved automatically to:
/MyDrive/tml_project/plots/


You can customize this path in `main.ipynb`.

---

## ğŸ”§ Requirements

- Python 3.8+
- PyTorch
- Matplotlib
- NumPy
- Google Colab (recommended)

---

## ğŸ“¬ Contact

**Author:** Arpitha Thippeswamy  
**Team Members:**
- Sree Aryan SP  
- Zhang Jiayi  
- Shashank Manjunatha


---





